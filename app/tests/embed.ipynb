{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "jina-embeddings-v3 is a multilingual multi-task text embedding model designed for a variety of NLP applications. Based on the Jina-XLM-RoBERTa architecture, this model supports Rotary Position Embeddings to handle long input sequences up to 8192 tokens. Additionally, it features 5 LoRA adapters to generate task-specific embeddings efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Tests fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the project's root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add the project root to the Python path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Now you can import directly\n",
    "from utils.embedding import EmbeddingGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import google.generativeai as genai\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from utils.embedding import EmbeddingGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"AIzaSyAker6F4E8-U6drPx76tC8gFHv1dU9I2Ww\"\n",
    "client = genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"postgresql+psycopg://postgres:postgres@localhost:5432/app\"\n",
    "collection_name = \"Info\"\n",
    "query = \"Hello CloudTuner, how are you doing?\"\n",
    "# # query = \"Tell me about cloudtuner?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Instantiate the Embeddings Class\n",
    "embedding_function = EmbeddingGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 5: Define and run the similarity search function\n",
    "def perform_similarity_search(query: str, top_k: int = 1):\n",
    "    vector_store = PGVector(\n",
    "        embeddings=embedding_function,\n",
    "        connection=connection_string,\n",
    "        collection_name=collection_name,\n",
    "        use_jsonb=True,\n",
    "    )\n",
    "    \n",
    "    results = vector_store.similarity_search(\n",
    "        query,\n",
    "        k=top_k,\n",
    "        score_threshold=1.0,\n",
    "        filter={\"source\": \"faq\"}\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = perform_similarity_search(query, top_k=1)\n",
    "final_content = \"\\n\\n\".join([doc.page_content for doc in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Content:\n",
      "There is no universal answer since the best performance-to-cost ratio depends on the specific characteristics of your workload. AWS and GCP are often preferred for compute-heavy tasks due to extensive instance portfolios and competitive spot pricing. Azure might be ideal for organizations leveraging Microsoft services and hybrid environments, whereas Alibaba could present attractive pricing for data-intensive or region-specific workloads. Cost simulation tools and dynamic recommendation engines are vital for profiling workloads and determining the optimal provider based on current cost and performance data.\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Content:\")\n",
    "print(final_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Search Results:\n",
      "An error occurred during similarity search: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Execute the search and print results\n",
    "try:\n",
    "    results = perform_similarity_search(query, top_k=1)\n",
    "    print(\"\\nSimilarity Search Results:\")\n",
    "    for document, score in results:\n",
    "        print(\"Document:\", document)\n",
    "        print(\"Relevance Score:\", score, \"\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during similarity search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-time applications require low latency, rapid scalability, and tight service integration. On AWS, EKS is ideal for ecosystems built on AWS with extensive native services. GCP's GKE is known for its straightforward setup and excellent scalability, particularly when integrated with advanced data analytics services. Azure’s AKS is optimal for organizations embedded in the Microsoft ecosystem, providing seamless integration with Azure AD and related tools. Refer to the respective E2E guides for Kubernetes, AWS, Azure, and GCP for specific trade-offs and optimizations. The decision is based on the need for operational efficiency versus granular control. EC2 is best when full control over instances is necessary, allowing complete environment customization. However, if managing infrastructure becomes burdensome or if your application benefits from automated scalability and reduced operational overhead, migrating to ECS/Fargate is recommended. This is especially true for microservices or stateless applications that can effectively utilize container orchestration. Savings Plans provide predictable cost savings for steady and predictable workloads, but they require a long-term commitment (one to three years). Conversely, Spot Instances offer significant discounts—often up to 90%—but come with the risk of interruption. For batch processing workloads that are fault-tolerant and can handle restarts, Spot Instances are typically the most cost-effective. The choice ultimately depends on whether workload stability or cost-savings under interruption risks is the priority. For short-duration workloads, the on-demand, pay-per-use pricing model is optimal because it strictly charges for the compute time used. This avoids the cost overhead of reserved instances and is particularly suitable for transient or bursty jobs running under 30 minutes. The Alibaba Cloud E2E guide highlights on-demand pricing as the most effective approach for such workloads. The best instance type depends on your workload’s requirements for performance and cost. For light-to-moderate ML inference, general-purpose burstable instances such as the T3 or T4 series are recommended. If GPU acceleration is essential, consider using smaller GPU instances available via spot pricing to keep monthly costs under $50. Regularly revisiting instance recommendations using dynamic cost-analysis and recommendation tools can help ensure you always use the best instance type relative to current pricing. There is no universal answer since the best performance-to-cost ratio depends on the specific characteristics of your workload. AWS and GCP are often preferred for compute-heavy tasks due to extensive instance portfolios and competitive spot pricing.\n"
     ]
    }
   ],
   "source": [
    "results = perform_similarity_search(query, top_k=1)\n",
    "merged_content = \"\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "print(merged_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Select Export type as in the report configured earlier: AWS Billing and Cost Management → Data Exports → find the report configured earlier → export type. Switch off Automatically detect existing Data Exports. Select Create new Data Export. Provide Data Export parameters: Export Name: enter a new name for the data export. Export Amazon S3 Bucket Name: AWS Billing and Cost Management → Data Exports table → S3 bucket column. Export path prefix: enter a new export path prefix that you want to prepend to the names of your report files. Note Specify the bucket in the \\'Export S3 Bucket Name\\' field if it already exists.Cloudtuner will then create the report and store it in the bucket using the specified prefix. Click Connect when done. Wait for AWS to generate the export and upload it to Cloudtuner according to the schedule (approximately one day). Warning AWS updates or creates a new export file once a day. If the export file is not placed in the specified bucket under the specified prefix, the export will fail with an error. Root account – Create Standard Data Export / Legacy CUR Export# Note Creating a data export is only available for the Root cloud account (payer), while all its Linked accounts will be centrally managed and receive their billing data through the main account\\'s invoice. In order to utilize automatic / manual billing data import in Cloudtuner, first, create a Data Export in AWS. Please refer to their official documentation to become acquainted with the guidelines for Data Exports. Navigate to AWS Billing & Cost Management → Data Exports. Create a new data export: Standard: 1. Select Standard data export as the export type. 2. Enter the export name. 3. Select CUR 2.0 → select the Include resource IDs checkbox → choose the time granularity for how you want the line items in the export to be aggregated. 4. Select Overwrite existing data export file → choose the compression type. 5. Set the data export storage setting: Create a new or use an existing bucket for the export. Enter the S3 path prefix that you want prepended to the name of your data export. 6. Confirm export creation. Data export is prepared by AWS within 24 hours. Legacy CUR Export: 1. Select Legacy CUR export (CUR) as the export type. 2. Enter the export name. 3.\\n\\nInsert the JSON code on the Type or paste a JSON policy document step: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CloudtunerOperations\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetBucketPublicAccessBlock\", \"s3:GetBucketPolicyStatus\", \"s3:GetBucketTagging\", \"iam:GetAccessKeyLastUsed\", \"cloudwatch:GetMetricStatistics\", \"s3:GetBucketAcl\", \"ec2:Describe*\", \"s3:ListAllMyBuckets\", \"iam:ListUsers\", \"s3:GetBucketLocation\", \"iam:GetLoginProfile\", \"cur:DescribeReportDefinitions\", \"iam:ListAccessKeys\" ], \"Resource\": \"*\" } ] } 4. Create user and grant policies: Go to Identity and Access Management (IAM) → Users → create a new user. In Step 2. Set permissions, select Attach policies directly → attach the policies created earlier. Confirm the creation of the user. 5. Create access key: Go to Identity and Access Management (IAM) → Users → select the created user → create an access key Download the .csv file with Access key and Secret access key. Connect to Cloudtuner# Once the user is configured, add the data source to Cloudtuner. Go to Cloudtuner → Data Sources → click the Add button → select AWS Root. Fill in the fields: Provide user credentials: AWS Access key ID → Access key, AWS Secret access key → Access key secret. Select Export type as in the report configured earlier: AWS Billing and Cost Management → Data Exports → find the report configured earlier → export type. Switch off Automatically detect existing Data Exports. Select Connect only to data in bucket. Provide Data Export parameters: Export Name: AWS Billing and Cost Management → Data Exports table → Export name column. Export Amazon S3 Bucket Name: AWS Billing and Cost Management → Data Exports table → S3 bucket column. Export path prefix: AWS Billing and Cost Management → Data Exports table → click on Export name → Edit → Data export storage settings → S3 destination → last folder name(without “/”). Example, S3 destination: s3://aqa-bill-bucket/report-cur2, enter report-cur2 into the field. Attention Wait for the export to be generated by AWS and uploaded to Cloudtuner according to the schedule (which is performed on an hourly basis). Please contact our Support Team at support@hystax.com if you have any questions regarding the described configuration flow. Root account – Data Export not configured yet# Cloudtuner supports the AWS Organizations service that allows linking several Data Sources in order to centrally manage data of multiple users while receiving all billing reports within a single invoice. The Root account (payer) will be the only one having access to collective data related to cloud spendings.\\n\\nSelect the Include resource IDs and Refresh automatically checkboxes. 4. Set the data export delivery options: Choose the time granularity for how you want the line items in the export to be aggregated. Sekect Overwrite existing report. Choose the compression type. 5. Set the data export storage setting: Create a new bucket or use an existing one for the export. Enter the S3 path prefix that you want prepended to the name of your data export. 6. Confirm export creation. Data export is prepared by AWS within 24 hours When it\\'s done, follow the steps from the section Root account – Data Export already configured Linked# Cloudtuner supports the AWS Organizations service that allows linking several Data Sources in order to centrally manage data of multiple users while receiving all billing exports within a single invoice. Configure policies# Before establishing the connection, include the Discover Resources policy to allow Cloudtuner to parse EC2 resource data: Follow steps 1-5 of the instructions. Insert the JSON code in the Type or paste a JSON policy document step: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CloudtunerOperations\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetBucketPublicAccessBlock\", \"s3:GetBucketPolicyStatus\", \"s3:GetBucketTagging\", \"iam:GetAccessKeyLastUsed\", \"cloudwatch:GetMetricStatistics\", \"s3:GetBucketAcl\", \"ec2:Describe*\", \"s3:ListAllMyBuckets\", \"iam:ListUsers\", \"s3:GetBucketLocation\", \"iam:GetLoginProfile\", \"cur:DescribeReportDefinitions\", \"iam:ListAccessKeys\" ], \"Resource\": \"*\" } ] } Now, your AWS Data Source is ready for integration with Cloudtuner! Connect to Cloudtuner# Go to Cloudtuner and select AWS Linked to simplify the registration process. This option removes the need to manually input bucket information for billing purposes, as the data will be received through the root account. The root user can then distribute periodic reports individually if required by company management. In this case, only the Access Key and Secret Access Key are needed. Note If you only specify a AWS Linked account without providing credentials for the main one, Cloudtuner is not able to import any billing data. Use Connect to create a Data Source in Cloudtuner. If some of the provided values are invalid, an error message indicates a failure to connect. Please contact our Support Team at support@hystax.com if you have any questions regarding the described configuration flow. Migrating from CUR to CUR 2.0# The information on this page can be useful if an AWS Data Source (Legacy CUR export schema) has already been connected and you want to configure CUR 2.0 data and update the AWS Data Source.\\n\\nAWS# Root account – Data Export already configured# Cloudtuner supports the AWS Organizations service that allows linking several Data Sources in order to centrally manage data of multiple users while receiving all billing exports within a single invoice. The Root account (payer) will be the only one having access to collective data related to cloud spendings. When registering this type of profile in Cloudtuner, the user is given an option for Data Exports to be detected automatically. Warning When you connect the root account but do not connect the linked accounts, all expenses from the unconnected linked accounts will be ignored, even if they exist in the data export file. To retrieve expenses from both linked and root accounts, connect all AWS accounts (not just the root).Cloudtuner ignores data from unconnected linked accounts. Configure policies and user# 1. Configure Data Exports. Having Data Exports configured for your cloud account is the main prerequisite in order to proceed with the remaining actions. If Data Export hasn\\'t been configured, refer to the Root Account – Data Export not configured yet instruction. 2. Update bucket policy: Navigate to the Permissions tab of your AWS S3 bucket → select Bucket Policy. Click + Add new statement → insert a JSON code snippet: { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Sid\":\"EnableAWSDataExportsToWriteToS3AndCheckPolicy\", \"Effect\":\"Allow\", \"Principal\":{ \"Service\":[ \"billingreports.amazonaws.com\", \"bcm-data-exports.amazonaws.com\" ] }, \"Action\":[ \"s3:PutObject\", \"s3:GetBucketPolicy\" ], \"Resource\":[ \"arn:aws:s3:::<bucket_name>/*\", \"arn:aws:s3:::<bucket_name>\" ], \"Condition\":{ \"StringLike\":{ \"aws:SourceAccount\":\"<AWS account ID>\", \"aws:SourceArn\":[ \"arn:aws:cur:us-east-1:<AWS account ID>:definition/*\", \"arn:aws:bcm-data-exports:us-east-1:<AWS account ID>:export/*\" ] } } } ] } Replace <bucket_name> with the name of the bucket. Replace <AWS account ID> with the AWS Account ID (12 digits without “-”). Save. 3. Create user policies for Discover Resources and ReadOnly access. ReadOnly access: Follow steps 1–5 of the instructions. Insert the JSON code in the Type or paste a JSON policy document step: { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Sid\":\"ReportDefinition\", \"Effect\":\"Allow\", \"Action\":[ \"cur:DescribeReportDefinitions\" ], \"Resource\":\"*\" }, { \"Sid\":\"GetObject\", \"Effect\":\"Allow\", \"Action\":[ \"s3:GetObject\" ], \"Resource\":\"arn:aws:s3:::<bucket_name>/*\" }, { \"Sid\":\"BucketOperations\", \"Effect\":\"Allow\", \"Action\":[ \"s3:ListBucket\", \"s3:GetBucketLocation\" ], \"Resource\":\"arn:aws:s3:::<bucket_name>\" } ] } Replace <bucket_name> with the name of the bucket created on the previous step. Discover Resources: Include the following policy to allow Cloudtuner to parse EC2 resource data. Follow steps 1-5 of the instructions.', 'question': 'how do i connect my aws account for billing information where i havent configured the data export'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_data = {\n",
    "    \"content\": merged_content,\n",
    "    \"question\": query\n",
    "}\n",
    "print(prompt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"Select Export type as in the report configured earlier: AWS Billing and Cost Management \\u2192 Data Exports \\u2192 find the report configured earlier \\u2192 export type. Switch off Automatically detect existing Data Exports. Select Create new Data Export. Provide Data Export parameters: Export Name: enter a new name for the data export. Export Amazon S3 Bucket Name: AWS Billing and Cost Management \\u2192 Data Exports table \\u2192 S3 bucket column. Export path prefix: enter a new export path prefix that you want to prepend to the names of your report files. Note Specify the bucket in the 'Export S3 Bucket Name' field if it already exists.Cloudtuner will then create the report and store it in the bucket using the specified prefix. Click Connect when done. Wait for AWS to generate the export and upload it to Cloudtuner according to the schedule (approximately one day). Warning AWS updates or creates a new export file once a day. If the export file is not placed in the specified bucket under the specified prefix, the export will fail with an error. Root account \\u2013 Create Standard Data Export / Legacy CUR Export# Note Creating a data export is only available for the Root cloud account (payer), while all its Linked accounts will be centrally managed and receive their billing data through the main account's invoice. In order to utilize automatic / manual billing data import in Cloudtuner, first, create a Data Export in AWS. Please refer to their official documentation to become acquainted with the guidelines for Data Exports. Navigate to AWS Billing & Cost Management \\u2192 Data Exports. Create a new data export: Standard: 1. Select Standard data export as the export type. 2. Enter the export name. 3. Select CUR 2.0 \\u2192 select the Include resource IDs checkbox \\u2192 choose the time granularity for how you want the line items in the export to be aggregated. 4. Select Overwrite existing data export file \\u2192 choose the compression type. 5. Set the data export storage setting: Create a new or use an existing bucket for the export. Enter the S3 path prefix that you want prepended to the name of your data export. 6. Confirm export creation. Data export is prepared by AWS within 24 hours. Legacy CUR Export: 1. Select Legacy CUR export (CUR) as the export type. 2. Enter the export name. 3.\\n\\nInsert the JSON code on the Type or paste a JSON policy document step: { \\\"Version\\\": \\\"2012-10-17\\\", \\\"Statement\\\": [ { \\\"Sid\\\": \\\"CloudtunerOperations\\\", \\\"Effect\\\": \\\"Allow\\\", \\\"Action\\\": [ \\\"s3:GetBucketPublicAccessBlock\\\", \\\"s3:GetBucketPolicyStatus\\\", \\\"s3:GetBucketTagging\\\", \\\"iam:GetAccessKeyLastUsed\\\", \\\"cloudwatch:GetMetricStatistics\\\", \\\"s3:GetBucketAcl\\\", \\\"ec2:Describe*\\\", \\\"s3:ListAllMyBuckets\\\", \\\"iam:ListUsers\\\", \\\"s3:GetBucketLocation\\\", \\\"iam:GetLoginProfile\\\", \\\"cur:DescribeReportDefinitions\\\", \\\"iam:ListAccessKeys\\\" ], \\\"Resource\\\": \\\"*\\\" } ] } 4. Create user and grant policies: Go to Identity and Access Management (IAM) \\u2192 Users \\u2192 create a new user. In Step 2. Set permissions, select Attach policies directly \\u2192 attach the policies created earlier. Confirm the creation of the user. 5. Create access key: Go to Identity and Access Management (IAM) \\u2192 Users \\u2192 select the created user \\u2192 create an access key Download the .csv file with Access key and Secret access key. Connect to Cloudtuner# Once the user is configured, add the data source to Cloudtuner. Go to Cloudtuner \\u2192 Data Sources \\u2192 click the Add button \\u2192 select AWS Root. Fill in the fields: Provide user credentials: AWS Access key ID \\u2192 Access key, AWS Secret access key \\u2192 Access key secret. Select Export type as in the report configured earlier: AWS Billing and Cost Management \\u2192 Data Exports \\u2192 find the report configured earlier \\u2192 export type. Switch off Automatically detect existing Data Exports. Select Connect only to data in bucket. Provide Data Export parameters: Export Name: AWS Billing and Cost Management \\u2192 Data Exports table \\u2192 Export name column. Export Amazon S3 Bucket Name: AWS Billing and Cost Management \\u2192 Data Exports table \\u2192 S3 bucket column. Export path prefix: AWS Billing and Cost Management \\u2192 Data Exports table \\u2192 click on Export name \\u2192 Edit \\u2192 Data export storage settings \\u2192 S3 destination \\u2192 last folder name(without \\u201c/\\u201d). Example, S3 destination: s3://aqa-bill-bucket/report-cur2, enter report-cur2 into the field. Attention Wait for the export to be generated by AWS and uploaded to Cloudtuner according to the schedule (which is performed on an hourly basis). Please contact our Support Team at support@hystax.com if you have any questions regarding the described configuration flow. Root account \\u2013 Data Export not configured yet# Cloudtuner supports the AWS Organizations service that allows linking several Data Sources in order to centrally manage data of multiple users while receiving all billing reports within a single invoice. The Root account (payer) will be the only one having access to collective data related to cloud spendings.\\n\\nSelect the Include resource IDs and Refresh automatically checkboxes. 4. Set the data export delivery options: Choose the time granularity for how you want the line items in the export to be aggregated. Sekect Overwrite existing report. Choose the compression type. 5. Set the data export storage setting: Create a new bucket or use an existing one for the export. Enter the S3 path prefix that you want prepended to the name of your data export. 6. Confirm export creation. Data export is prepared by AWS within 24 hours When it's done, follow the steps from the section Root account \\u2013 Data Export already configured Linked# Cloudtuner supports the AWS Organizations service that allows linking several Data Sources in order to centrally manage data of multiple users while receiving all billing exports within a single invoice. Configure policies# Before establishing the connection, include the Discover Resources policy to allow Cloudtuner to parse EC2 resource data: Follow steps 1-5 of the instructions. Insert the JSON code in the Type or paste a JSON policy document step: { \\\"Version\\\": \\\"2012-10-17\\\", \\\"Statement\\\": [ { \\\"Sid\\\": \\\"CloudtunerOperations\\\", \\\"Effect\\\": \\\"Allow\\\", \\\"Action\\\": [ \\\"s3:GetBucketPublicAccessBlock\\\", \\\"s3:GetBucketPolicyStatus\\\", \\\"s3:GetBucketTagging\\\", \\\"iam:GetAccessKeyLastUsed\\\", \\\"cloudwatch:GetMetricStatistics\\\", \\\"s3:GetBucketAcl\\\", \\\"ec2:Describe*\\\", \\\"s3:ListAllMyBuckets\\\", \\\"iam:ListUsers\\\", \\\"s3:GetBucketLocation\\\", \\\"iam:GetLoginProfile\\\", \\\"cur:DescribeReportDefinitions\\\", \\\"iam:ListAccessKeys\\\" ], \\\"Resource\\\": \\\"*\\\" } ] } Now, your AWS Data Source is ready for integration with Cloudtuner! Connect to Cloudtuner# Go to Cloudtuner and select AWS Linked to simplify the registration process. This option removes the need to manually input bucket information for billing purposes, as the data will be received through the root account. The root user can then distribute periodic reports individually if required by company management. In this case, only the Access Key and Secret Access Key are needed. Note If you only specify a AWS Linked account without providing credentials for the main one, Cloudtuner is not able to import any billing data. Use Connect to create a Data Source in Cloudtuner. If some of the provided values are invalid, an error message indicates a failure to connect. Please contact our Support Team at support@hystax.com if you have any questions regarding the described configuration flow. Migrating from CUR to CUR 2.0# The information on this page can be useful if an AWS Data Source (Legacy CUR export schema) has already been connected and you want to configure CUR 2.0 data and update the AWS Data Source.\\n\\nAWS# Root account \\u2013 Data Export already configured# Cloudtuner supports the AWS Organizations service that allows linking several Data Sources in order to centrally manage data of multiple users while receiving all billing exports within a single invoice. The Root account (payer) will be the only one having access to collective data related to cloud spendings. When registering this type of profile in Cloudtuner, the user is given an option for Data Exports to be detected automatically. Warning When you connect the root account but do not connect the linked accounts, all expenses from the unconnected linked accounts will be ignored, even if they exist in the data export file. To retrieve expenses from both linked and root accounts, connect all AWS accounts (not just the root).Cloudtuner ignores data from unconnected linked accounts. Configure policies and user# 1. Configure Data Exports. Having Data Exports configured for your cloud account is the main prerequisite in order to proceed with the remaining actions. If Data Export hasn't been configured, refer to the Root Account \\u2013 Data Export not configured yet instruction. 2. Update bucket policy: Navigate to the Permissions tab of your AWS S3 bucket \\u2192 select Bucket Policy. Click + Add new statement \\u2192 insert a JSON code snippet: { \\\"Version\\\":\\\"2012-10-17\\\", \\\"Statement\\\":[ { \\\"Sid\\\":\\\"EnableAWSDataExportsToWriteToS3AndCheckPolicy\\\", \\\"Effect\\\":\\\"Allow\\\", \\\"Principal\\\":{ \\\"Service\\\":[ \\\"billingreports.amazonaws.com\\\", \\\"bcm-data-exports.amazonaws.com\\\" ] }, \\\"Action\\\":[ \\\"s3:PutObject\\\", \\\"s3:GetBucketPolicy\\\" ], \\\"Resource\\\":[ \\\"arn:aws:s3:::<bucket_name>/*\\\", \\\"arn:aws:s3:::<bucket_name>\\\" ], \\\"Condition\\\":{ \\\"StringLike\\\":{ \\\"aws:SourceAccount\\\":\\\"<AWS account ID>\\\", \\\"aws:SourceArn\\\":[ \\\"arn:aws:cur:us-east-1:<AWS account ID>:definition/*\\\", \\\"arn:aws:bcm-data-exports:us-east-1:<AWS account ID>:export/*\\\" ] } } } ] } Replace <bucket_name> with the name of the bucket. Replace <AWS account ID> with the AWS Account ID (12 digits without \\u201c-\\u201d). Save. 3. Create user policies for Discover Resources and ReadOnly access. ReadOnly access: Follow steps 1\\u20135 of the instructions. Insert the JSON code in the Type or paste a JSON policy document step: { \\\"Version\\\":\\\"2012-10-17\\\", \\\"Statement\\\":[ { \\\"Sid\\\":\\\"ReportDefinition\\\", \\\"Effect\\\":\\\"Allow\\\", \\\"Action\\\":[ \\\"cur:DescribeReportDefinitions\\\" ], \\\"Resource\\\":\\\"*\\\" }, { \\\"Sid\\\":\\\"GetObject\\\", \\\"Effect\\\":\\\"Allow\\\", \\\"Action\\\":[ \\\"s3:GetObject\\\" ], \\\"Resource\\\":\\\"arn:aws:s3:::<bucket_name>/*\\\" }, { \\\"Sid\\\":\\\"BucketOperations\\\", \\\"Effect\\\":\\\"Allow\\\", \\\"Action\\\":[ \\\"s3:ListBucket\\\", \\\"s3:GetBucketLocation\\\" ], \\\"Resource\\\":\\\"arn:aws:s3:::<bucket_name>\\\" } ] } Replace <bucket_name> with the name of the bucket created on the previous step. Discover Resources: Include the following policy to allow Cloudtuner to parse EC2 resource data. Follow steps 1-5 of the instructions.\",\n",
      "  \"question\": \"how do i connect my aws account for billing information where i havent configured the data export\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "combined_prompt = json.dumps(prompt_data, indent=2)\n",
    "print(combined_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of prompt.txt\n",
    "with open(\"prompt.txt\", \"r\") as file:\n",
    "    prompt_content = file.read()\n",
    "\n",
    "print(prompt_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = prompt_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction,),\n",
    "    contents=combined_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stream_response = client.models.generate_content_stream(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction,),\n",
    "    contents=combined_prompt,\n",
    ")\n",
    "for chunk in stream_response:\n",
    "    print(chunk.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Database Table Similarity Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
