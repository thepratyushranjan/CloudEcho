{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "jina-embeddings-v3 is a multilingual multi-task text embedding model designed for a variety of NLP applications. Based on the Jina-XLM-RoBERTa architecture, this model supports Rotary Position Embeddings to handle long input sequences up to 8192 tokens. Additionally, it features 5 LoRA adapters to generate task-specific embeddings efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Tests fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the project's root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add the project root to the Python path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Now you can import directly\n",
    "from utils.embedding import EmbeddingGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import google.generativeai as genai\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from utils.embedding import EmbeddingGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"AIzaSyAker6F4E8-U6drPx76tC8gFHv1dU9I2Ww\"\n",
    "client = genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"postgresql+psycopg://postgres:postgres@localhost:5432/app\"\n",
    "collection_name = \"Info\"\n",
    "query = \"What is cloudtuner how confgure aws and export billing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Instantiate the Embeddings Class\n",
    "embedding_function = EmbeddingGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 5: Define and run the similarity search function\n",
    "def perform_similarity_search(query: str, top_k: int = 1, min_score: float = 0.1):\n",
    "    vector_store = PGVector(\n",
    "        embeddings=embedding_function,\n",
    "        connection=connection_string,\n",
    "        collection_name=collection_name,\n",
    "        use_jsonb=True,\n",
    "    )\n",
    "    \n",
    "    results_with_scores = vector_store.similarity_search_with_score(\n",
    "        query,\n",
    "        k=top_k,\n",
    "        # score_threshold=1.0,  # The built-in filtering threshold may be adjusted too.\n",
    "        filter={\"source\": \"faq\"}\n",
    "    )\n",
    "    filtered_results = [(doc, score) for doc, score in results_with_scores if score >= min_score]\n",
    "\n",
    "    return filtered_results if filtered_results else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = perform_similarity_search(query, top_k=1)\n",
    "final_content = \"\\n\\n\".join([doc.page_content for doc in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Content:\")\n",
    "print(final_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Search Results:\n",
      "An error occurred during similarity search: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Execute the search and print results\n",
    "try:\n",
    "    results = perform_similarity_search(query, top_k=1, min_score=1.0)\n",
    "    print(\"\\nSimilarity Search Results:\")\n",
    "    if not results:\n",
    "        print(\"No exact FAQ match found.\")\n",
    "    else:\n",
    "        for document, score in results:\n",
    "            print(\"Document:\", document)\n",
    "            print(\"Relevance Score:\", score, \"\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during similarity search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-time applications require low latency, rapid scalability, and tight service integration. On AWS, EKS is ideal for ecosystems built on AWS with extensive native services. GCP's GKE is known for its straightforward setup and excellent scalability, particularly when integrated with advanced data analytics services. Azure’s AKS is optimal for organizations embedded in the Microsoft ecosystem, providing seamless integration with Azure AD and related tools. Refer to the respective E2E guides for Kubernetes, AWS, Azure, and GCP for specific trade-offs and optimizations. The decision is based on the need for operational efficiency versus granular control. EC2 is best when full control over instances is necessary, allowing complete environment customization. However, if managing infrastructure becomes burdensome or if your application benefits from automated scalability and reduced operational overhead, migrating to ECS/Fargate is recommended. This is especially true for microservices or stateless applications that can effectively utilize container orchestration. Savings Plans provide predictable cost savings for steady and predictable workloads, but they require a long-term commitment (one to three years). Conversely, Spot Instances offer significant discounts—often up to 90%—but come with the risk of interruption. For batch processing workloads that are fault-tolerant and can handle restarts, Spot Instances are typically the most cost-effective. The choice ultimately depends on whether workload stability or cost-savings under interruption risks is the priority. For short-duration workloads, the on-demand, pay-per-use pricing model is optimal because it strictly charges for the compute time used. This avoids the cost overhead of reserved instances and is particularly suitable for transient or bursty jobs running under 30 minutes. The Alibaba Cloud E2E guide highlights on-demand pricing as the most effective approach for such workloads. The best instance type depends on your workload’s requirements for performance and cost. For light-to-moderate ML inference, general-purpose burstable instances such as the T3 or T4 series are recommended. If GPU acceleration is essential, consider using smaller GPU instances available via spot pricing to keep monthly costs under $50. Regularly revisiting instance recommendations using dynamic cost-analysis and recommendation tools can help ensure you always use the best instance type relative to current pricing. There is no universal answer since the best performance-to-cost ratio depends on the specific characteristics of your workload. AWS and GCP are often preferred for compute-heavy tasks due to extensive instance portfolios and competitive spot pricing.\n"
     ]
    }
   ],
   "source": [
    "results = perform_similarity_search(query, top_k=1)\n",
    "merged_content = \"\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "print(merged_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_data = {\n",
    "    \"content\": merged_content,\n",
    "    \"question\": query\n",
    "}\n",
    "print(prompt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "combined_prompt = json.dumps(prompt_data, indent=2)\n",
    "print(combined_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of prompt.txt\n",
    "with open(\"prompt.txt\", \"r\") as file:\n",
    "    prompt_content = file.read()\n",
    "\n",
    "print(prompt_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = prompt_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction,),\n",
    "    contents=combined_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stream_response = client.models.generate_content_stream(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction,),\n",
    "    contents=combined_prompt,\n",
    ")\n",
    "for chunk in stream_response:\n",
    "    print(chunk.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Database Table Similarity Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
